# deck: ml
* Example (ML):
  A particular instance of data
* Labeled example (ML):
  Example with correct label, used to train
* Unlabeled example (ML):
  Example w/o label, predicting label is the point
* Model (ML):
  Mapping from examples to predicted labels, parameters are learned
* Label (ML):
  Output, what is predicted
* Feature (ML):
  Input variable
* Inference (ML):
  Applying the model to unlabeled examples
* Regression model (ML):
  Predicts continuous values
* Classification model (ML):
  Predicts discrete values
* Bias (linear regression, ML):
  The y-intercept in 1D, shows y if x = 0 
* Weight (linear regression, ML):
  Like the slope, shows dependence of label on changing features
* Loss (ML):
  Measures badness of model on a particular example
* Mean square error, MSE (ML):
  Average squared loss per example over dataset
* Converged model (ML):
  A model where the loss has stopped changing with continued updates
* Requirement for convergence in optimization problems:
  Convexity of the loss function (equivalent to single minimum)
* Gradient descent (ML):
  Compute the gradient of the loss using a small number of examples then move along the negative gradient in the weight space
* Hyperparameters (ML):
  Knobs that programmers tweak
* Stochastic gradient descent, SGD:
  Gradient descent using only a single example at a time
* Feature column in tf:
  Construct describing datatype for input to TF
* If training error decreases slowly:
  Increase the learning rate
* If training error fluctuates a lot:
  Decrease the learning rate
* If training error has not plateaued:
  Increase number of steps
* Create a linear regressor in tf:
  tf.estimator.LinearRegressor(feature_cols_list, optimizer)
* Create a numeric feature column in tf:
  tf.feature_column.numeric_column('name_of_col')
* Create gradient descent optimizer in tf:
  tf.train.GradientDescentOptimizer(learning_rate=lr)
* Clip gradient descent optimizer in tf:
  tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_by)
* tf.data.Dataset:
  Comprised of elements that each contain one or more tf.Tensor objects
* Simplest way to get items from a Dataset:
  ds.make_one_shot_iterator().get_next()
* Actually display an item of data from a tf dataset iterator:
  sess.run(print(iterator.get_next()))
* tf.Tensor:
  Handle to an output of a tensorflow operation
* Train a model in tensorflow:
  model.train(input_fn=input_fn, steps=n_steps)
* Input function for training model in tensorflow:
  Function that provides data in the form of ({feature_label: feature_tensor}, label_tensor)
* Find variable names in a tf model:
  model.get_variable_names()
* Find variable values in a tf model, e.g. the biases in a linear model:
  model.get_variable_value(name), e.g. 'linear/linear_model/bias_weights'
* Create tf Dataset with elements ({'xs': tensor_x}, tensor_y):
  tf.data.Dataset.from_tensor_slices(({'xs': xs}, ys))
* Models generalize well when:
  Examples are iid, distribution is stationary, examples from same distribution
* Validation set:
  Set of examples to test and get hyperparameters, not training
* Three sets when partitioning examples:
  Training, validation, test
* Condition to trust model will generalize:
  Validation and test loss are similar
* Module, syntax to get mean squared error:
  sklearn.metrics.mean_squared_error(predictions, targets)
* Produce predictions using tensorflow:
  Call model.predict(input_fn=predict_fn), predict_fn does no shuffling, batch with single item, only repeat one epoch, only returns data
* Feature engineering:
  Transforming raw data into a feature vector
* One-hot encoding:
  One position of binary array is 1, representing "active" category
* Multi-hot encoding:
  Multiple values in string are 1, multiple "active" categories
* Steps to take in preparing data:
  Look at distribution, avoid features with few/one member, remove/clip outliers, think of binning, remove magic values, make meaning clear, scale
* Z score:
  (value - mean)/std
* Possibles errors in data:
  Missing/incorrect data and duplicates
* Usefulness of feature crosses:
  Allows nonlinearity in linear model, so generalize well
* Feature cross of one-hot encoded data:
  Actually given by the tensor product
* argmax(f):
  The value of the argument of f where it is maximized (in domain of f)
* Dropout is likely useful:
  With dense features
* Create a bucketized feature column:
  tf.feature_column.bucketized_column(numeric_column, boundaries)
* Is it necessary to create bucketized columns in the dataframe:
  No, tensorflow will handle it for you.
* Create a crossed column:
  tf.feature_column.crossed_column(cols_to_cross, hash_bucket_size)
* hash_bucket_size in crossed column:
  Basically, crossed feature are transformed to hash(cartesian product of features) % bucket_size
* Validation loss begins to increase while training loss decreases:
  Starting to overfit
* Extra term in loss function that only knows about weights:
  A regularizer
* Regularizing the loss function:
  Helps control model complexity to avoid overfitting
* L2 regularizer:
  Keeps the weights as small as possible --- prior is zero weights
* Cases where you are more likely to need strong reglarization:
  Whenever overfitting is a danger, e.g. small, noisy data sets
* Regularization in logistic models:
  Is very important, because in high dimensions tries hard to drive loss to zero, need to penalize large weights
* Reasons to use linear logistic regression:
  Very fast training/prediction, so good for large amounts of data
* Sigmoid function with linear input:
  1/(1+exp(-z)), z = bias + weights*xs
* z in sigmoid is called the log-odds:
  Because it is given by z = log(p/(1-p)), p is output probability, so looks like odds, p(happens)/p(not happens)
* Logistic loss function (log loss):
  -y log(y') - (1-y) log(1-y') where y' is sigmoid of log-odds
* Accuracy:
  (correct predictions)/(total predictions)
* In true/false positive/negative:
  Refers to prediction --- actual is opposite for false
* Accuracy can be misleading for evaluating categorical model:
  If there is a class imbalance
* Class imbalance:
  When one categorical outcome is very rare compared to others
* Precision:
  (True positives)/(All positive predictions) --- avoid crying wolf
* Recall:
  (True positives)/(Actual positives) --- also called true positive rate
* Effect of raising classification threshold on precision:
  Increases
* Effect of raising classification threshold on recall:
  Decreases
* Classification or prediction threshold:
  Used to decide what category a particular instance should be in given a probability from model
* ROC curve:
  Receiver operating characteristics curve, how model does with all thresholds, true positives rate vs false positive rate
* AUC (or AUROC):
  Area under ROC curve, probability that model gets pairwise ranking of positive, negative correct (does not depend on threshold)
* Prediction bias:
  Difference between sum of things we predict and sum of things we observe
* Harmonic mean:
  Reciprocal of arithmetic mean of reciprocals (makes sense for rates)
* F1 score:
  Harmonic mean of the recall and precision
* False negatives hurt:
  Recall
* False positives hurt:
  Precision
* True positive rate:
  Same as the recall, (true positive)/(actual positive)
* False positive rate:
  (false positve)/(actual negative)
* Direction of raising the classification threshold on the ROC curve:
  Down and left, because fewer items are classified as positive
* Steps to prove AUC = probability of correct ordering:
  Write TP, FP rates in terms of cutoff, change variables in AUC to cutoff, profit
* Does AUC depend on classification threshold:
  No
* AUC not as useful when:
  Making mistakes in one direction (false positives or false negatives) is much worse than the other, like cancer
* Prediction bias:
  Difference between average of predictions and average of observations
* Possible causes for prediction bias:
  BINBO: buggy pipeline, incomplete features, nosiy data, biased training set, overly strong regularization
* Calibration layer:
  Layer that removes prediction bias from model output
* Problems with calibration layer:
  Fixing symptom, not cause, and brittle
* Low prediction bias is (necessary/sufficient) to have a good model:
  Necessary but not sufficient
* You must bucekt predictions to see bias:
  Because output is binary, need to see ensemble
* Calibration plot:
  Shows the label vs prediction for buckets of sorted examples, to see bias at different values of predicition
* If model is badly calibrated for only some regions:
  Training data might not sample regions, some subsets are noisy, too regularized (or just look-elsewhere)
* Create linear classifier in tensorflow:
  model = tf.estimator.LinearClassifier(feature_columsn, optimizer=optimizer)
