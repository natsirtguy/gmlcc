# deck: ml
* Example (ML):
  A particular instance of data
* Labeled example (ML):
  Example with correct label, used to train
* Unlabeled example (ML):
  Example w/o label, predicting label is the point
* Model (ML):
  Mapping from examples to predicted labels, parameters are learned
* Label (ML):
  Output, what is predicted
* Feature (ML):
  Input variable
* Inference (ML):
  Applying the model to unlabeled examples
* Regression model (ML):
  Predicts continuous values
* Classification model (ML):
  Predicts discrete values
* Bias (linear regression, ML):
  The y-intercept in 1D, shows y if x = 0 
* Weight (linear regression, ML):
  Like the slope, shows dependence of label on changing features
* Loss (ML):
  Measures badness of model on a particular example
* Mean square error, MSE (ML):
  Average squared loss per example over dataset
* Converged model (ML):
  A model where the loss has stopped changing with continued updates
* Requirement for convergence in optimization problems:
  Convexity of the loss function (equivalent to single minimum)
* Gradient descent (ML):
  Compute the gradient of the loss using a small number of examples then move along the negative gradient in the weight space
* Hyperparameters (ML):
  Knobs that programmers tweak
* Stochastic gradient descent, SGD:
  Gradient descent using only a single example at a time
* Feature column in tf:
  Construct describing data type for input to TF
* If training error decreases slowly:
  Increase the learning rate
* If training error fluctuates a lot:
  Decrease the learning rate
* If training error has not plateaued:
  Increase number of steps
* Create a linear regressor in tf:
  tf.estimator.LinearRegressor(feature_cols_list, optimizer)
* Create a numeric feature column in tf:
  tf.feature_column.numeric_column('name_of_col')
* Create gradient descent optimizer in tf:
  tf.train.GradientDescentOptimizer(learning_rate=lr)
* Clip gradient descent optimizer in tf:
  tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_by)
* tf.data.Dataset:
  Comprised of elements that each contain one or more tf.Tensor objects
* Simplest way to get items from a Dataset:
  ds.make_one_shot_iterator().get_next()
* Actually display an item of data from a tf dataset iterator:
  sess.run(print(iterator.get_next()))
* tf.Tensor:
  Handle to an output of a tensorflow operation
* Train a model in tensorflow:
  model.train(input_fn=input_fn, steps=n_steps)
* Input function for training model in tensorflow:
  Function that provides data in the form of ({feature_label: feature_tensor}, label_tensor)
* Find variable names in a tf model:
  model.get_variable_names()
* Find variable values in a tf model, e.g. the biases in a linear model:
  model.get_variable_value(name), e.g. 'linear/linear_model/bias_weights'
* Create tf Dataset with elements ({'xs': tensor_x}, tensor_y):
  tf.data.Dataset.from_tensor_slices(({'xs': xs}, ys))
* Models generalize well when:
  Examples are iid, distribution is stationary, examples from same distribution
* Validation set:
  Set of examples to test and get hyperparameters, not training
* Three sets when partitioning examples:
  Training, validation, test
* Condition to trust model will generalize:
  Validation and test loss are similar
* Module, syntax to get mean squared error:
  sklearn.metrics.mean_squared_error(predictions, targets)
* Produce predictions using tensorflow:
  Call model.predict(input_fn=predict_fn), predict_fn does no shuffling, batch with single item, only repeat one epoch, only returns data
* Repeat=None in tf.data.Dataset means:
  That the data will repeat indefinitely, must set repeat=1 for predictions
* Feature engineering:
  Transforming raw data into a feature vector
* One-hot encoding:
  One position of binary array is 1, representing "active" category
* Multi-hot encoding:
  Multiple values in string are 1, multiple "active" categories
* Steps to take in preparing data:
  MUCBOMS: make meaning clear, understand distribution, clip outliers, bin, remove features with ONE/few members, magic values, scale
* Z score:
  (value - mean)/std
* Possibles errors in data:
  Missing/incorrect data and duplicates
* Usefulness of feature crosses:
  Allows nonlinearity in linear model, so generalize well
* Feature cross of one-hot encoded data:
  Actually given by the tensor product
* argmax(f):
  The value of the argument of f where it is maximized (in domain of f)
* Dropout is likely useful:
  With dense features
* Create a bucketized feature column:
  tf.feature_column.bucketized_column(numeric_column, boundaries)
* Is it necessary to create bucketized columns in the dataframe:
  No, tensorflow will handle it for you.
* Create a crossed column:
  tf.feature_column.crossed_column(cols_to_cross, hash_bucket_size)
* hash_bucket_size in crossed column:
  Basically, crossed feature are transformed to hash(Cartesian product of features) % bucket_size
* Validation loss begins to increase while training loss decreases:
  Starting to overfit
* Extra term in loss function that only knows about weights:
  A regularizer
* Regularizing the loss function:
  Helps control model complexity to avoid overfitting
* L2 regularizer:
  Keeps the weights as small as possible --- prior is zero weights
* Cases where you are more likely to need strong regularization:
  Whenever overfitting is a danger, e.g. small, noisy data sets
* Regularization in logistic models:
  Is very important, because in high dimensions tries hard to drive loss to zero, need to penalize large weights
* Reasons to use linear logistic regression:
  Very fast training/prediction, so good for large amounts of data
* Sigmoid function with linear input:
  1/(1+exp(-z)), z = bias + weights*xs
* z in sigmoid is called the log-odds:
  Because it is given by z = log(p/(1-p)), p is output probability, so looks like odds, p(happens)/p(not happens)
* Logistic loss function (log loss):
  -y log(y') - (1-y) log(1-y') where y' is sigmoid of log-odds
* Accuracy:
  (correct predictions)/(total predictions)
* In true/false positive/negative:
  Refers to prediction --- actual is opposite for false
* Accuracy can be misleading for evaluating categorical model:
  If there is a class imbalance
* Class imbalance:
  When one categorical outcome is very rare compared to others
* Precision:
  (True positives)/(All positive predictions) --- avoid crying wolf
* Recall:
  (True positives)/(Actual positives) --- also called true positive rate
* Effect of raising classification threshold on precision:
  Increases
* Effect of raising classification threshold on recall:
  Decreases
* Classification or prediction threshold:
  Used to decide what category a particular instance should be in given a probability from model
* ROC curve:
  Receiver operating characteristics curve, how model does with all thresholds, true positives rate vs false positive rate
* AUC (or AUROC):
  Area under ROC curve, probability that model gets pairwise ranking of positive, negative correct (does not depend on threshold)
* Harmonic mean:
  Reciprocal of arithmetic mean of reciprocals (makes sense for rates)
* F1 score:
  Harmonic mean of the recall and precision
* False negatives hurt:
  Recall
* False positives hurt:
  Precision
* True positive rate:
  Same as the recall, (true positive)/(actual positive)
* False positive rate:
  (false positive)/(actual negative)
* Direction of raising the classification threshold on the ROC curve:
  Down and left, because fewer items are classified as positive
* Steps to prove AUC = probability of correct ordering:
  Write TP, FP rates in terms of cutoff, change variables in AUC to cutoff, profit
* Does AUC depend on classification threshold:
  No
* AUC not as useful when:
  Making mistakes in one direction (false positives or false negatives) is much worse than the other, like cancer
* Prediction bias:
  Difference between average of predictions and average of observations
* Possible causes for prediction bias:
  Noisy pipes train incomplete regularizers: buggy pipeline, noisy data, biased training set, incomplete features, overly strong regularization
* Calibration layer:
  Layer that removes prediction bias from model output
* Problems with calibration layer:
  Fixing symptom, not cause, and brittle
* Low prediction bias is (necessary/sufficient) to have a good model:
  Necessary but not sufficient
* You must bucket predictions to see bias:
  Because output is binary, need to see ensemble
* Calibration plot:
  Shows the label vs prediction for buckets of sorted examples, to see bias at different values of prediction
* If model is badly calibrated for only some regions:
  Training data might not sample regions, some subsets are noisy, too regularized (or just look-elsewhere)
* Create linear classifier in tensorflow:
  model = tf.estimator.LinearClassifier(feature_columns, optimizer=optimizer)
* Log-odds is also called:
  logit
* Fields in dictionaries returned by linear_classifier.predict() with boolean category:
  logits, logistic, probabilities, classes, class_ids
* logistic in linear_classifier.predict() elements:
  The value of the sigmoid function for the given prediction
* probabilities in linear_classifier.predict() elements:
  Array of probabilities of the different classes, probabilities[0] is probability that it is false for Boolean, probabilities[1] is equivalent to logistic
* classes in linear_classifier.predict() elements:
  Array with the most likely class for the given prediction
* class_ids in linear_classifier.predict() elements:
  Array with the class id (an int) of most likely prediction, so element["probabilities"][element["class_ids"][0]] is the max of probabilities
* Evaluate the performance of a tensorflow model:
  model.evaluate(input_fn=validation_fn)
* Get true positive rate and false positive rate in python:
  fpr, tpr, thresholds = sklearn.metrics.roc_curve(true_labels, logistics)
* Problem with feature crosses in large, sparse data:
  The crossed features become very high dimensional and even sparser
* Does L2 regularization penalize nonzero weights:
  No, it drives weights to small but nonzero values (derivative shrinks)
* Is it possible to use "L0" regularization (penalize number of weights):
  Not really, becomes nonconvex and NP-hard
* L1 penalizes:
  The sum of the absolute value of the weights
* Possible drawback of L1 setting values to zero:
  If the true minimum of the loss function involves a nonzero weight and a local minima involves setting that to zero, the weight could get set to zero early in the training process and stay there
* Can we get nonlinearity be simply introducing additional layers:
  No, not if the layers are linearly combining input
* Simple nonlinearity sometimes used in neural nets:
  ReLU, rectified linear unit, just cuts off function at zero for negative values
* Why initialization can matter a lot for neural nets:
  Nonlinear means nonconvex optimization
* Another name for nonlinear transformation layer:
  Activation function, from neurons
* Tool to do gradient descent:
  Backpropagation
* Advantage of neural networks over feature crosses:
  More flexible, work in more cases, don't need to see the structure and create features
* Manifold hypothesis:
  Natural data forms lower-dimensional submanifolds in its embedding space
* Relationship between number of layers and topology:
  The number of layers is effectively the dimensionality of the embedding space, so increasing the dimensionality allows us to untangle more things
* Create a neural network regressor in tf with 3 nodes in first layer, 4 in second:
  tf.estimator.DNNRegressor([3,4], fcs, optimizer=optimizer)
* Keyword argument, default for activation function in DDNRegressor:
  activation_fn, tf.nn.relu
* Another word for loss function in neural networks:
  Error function
* Update rule involving error derivative:
  new weights = old weights - (learning rate)*(d Error)/(d weights)
* Backpropagation is basically just this from calculus:
  The chain rule --- we compute derivatives wrt weights by looking at partial derivatives involving outputs and activation functions
* Use of dynamic programming in neural networks:
  For backpropagation, allows you to avoid recomputing many derivatives
* Property of loss function for backpropagation to work:
  Must be differentiable
* Reason to keep number of layers small:
  Decreasing signal to noise ratio means useful gradients in backpropagation can shrink
* Useful to avoid small gradients:
  ReLU, because derivative is not scaled
* Batch normalization:
  Using the average gradient steps from several examples in a batch
* Three ways to avoid exploding gradients:
  Reduce learning rate, scale input features, use batch normalization
* Dropout:
  On a gradient step, basically ignore a given neuron with some probability
* Dropout is a form of regularization in that:
  In some sense it reduces the model complexity --- not using the whole model the whole time
* Ways backpropagation can fail:
  Vanishing/exploding gradients, ReLU failure
* Which neurons are likely to have small/large gradients:
  Closer to input, because product of many terms, which can be small or large
* ReLUs can die:
  If sum of inputs drop below zero (no contribution to gradient)
* Avoid dead ReLUs:
  Try reducing learning rate, change initialization
* Dynamic programming:
  Breaking a problem up into smaller problems that tend to repeat and memoizing the results to avoid recomputing
* Adagrad optimizer:
  Adaptive gradient descent, monotonically lowers the rate for each coefficient separately
* Softmax:
  Generalization of logistic regression assigning probabilities to different classes
* In Softmax, outputs sum to:
  One, so they can be interpreted as probabilities (also helps convergence)
* Softmax is better than many individual one-vs-all classifiers:
  Because we can share the internal state of the network between them
* When to use Softmax layer:
  A multi-class problem with a single label
* Number of neurons in Softmax layer:
  Same as number of classes/outputs
* Softmax equation:
  P(y = j|x) = exp(weight_j * x + b_j)/(sum_i exp(weight_i * x + b_i))
* Two variants of Softmax:
  Full Softmax and candidate sampling
* Candidate sampling in Softmax:
  When training, only compute the probabilities for positives labels and a random sample of negative labels
* If there are many positive labels in a multi-class problem:
  It doesn't make sense use Softmax
* Create feature column called "pixels" that will hold arrays with len 1600:
  tf.feature_column.numeric_column("pixels", shape=1600)
* sklearn.metrics.log_loss does/does not work with multiple classes:
  It does, just feed an array with dimension (n_examples, n_classes) as second argument (prediction)
* tensorflow function to one-hot encode array of integers:
  tf.keras.util.to_categorical(array, n_classes)
* Confusion matrix function:
  sklearn.metrics.confusion_matrix(targets, predictions)
* Embedding (concept):
  Representing features in some space so that similar features are nearby --- think of similar movies
* Creating dense representation of sparse features:
  First build dictionary to sparse features, then represent sparse vector as list containing keys to sparse features
* Coordinate of the feature in the embedding space is represented by:
  The weight from the feature to each node in the embedding layer
* The weight from feature to node in embedding layer:
  Corresponds to position in embedding space
* Rule-of-thumb for number of embedding dimensions/units:
  Fourth root of number of possible values
* Embedding is a useful tool by itself:
  Because it operationally groups features, showing relationships in data
* Collaborative filtering:
  Predicting user preferences based on other users
* Latent dimension:
  Feature inferred from data
* Problem with sparse input vectors:
  Large number of weights -> memory, computation limited
* When an embedding is useful:
  Highly dimensional space of features (like languages with lists of words) that could have structure to them (like meanings of words)
* Representing embedding as a matrix:
  With N vocabulary items, M embedding dimensions, write embedding as NxM matrix, so that left multiplying by one-hot vector gives dense vector
* PCA:
  Principal component analysis
* Picture of PCA:
  Fitting an ellipsoid to data, axes of ellipsoid are principal components
* Definition of PCA:
  Change of coordinates (orthogonal, linear) so that data varies most along first coordinate, next most along second, etc.
* Data matrix (input) for PCA:
  Columns are features adjusted to have zero empirical mean, rows are experiments
* Covariance matrix in terms of data matrix X:
  Proportional to X^T X, since <xy - <x><y>> = <xy> is covariance of features with mean zero
* PCA in terms of eigenvectors:
  Equivalent to eigenvector decomposition of covariance matrix
* Sample covariance between principal components:
  Zero, basically since eigenvectors are orthogonal or orthogonalizable
* Using PCA to reduce dimensionality:
  Use only a subset of principal components (eigenvectors) with largest variance
* Rayleigh quotient:
  Same as expectation value in QM of Hermitian matrix with given state vector
* Equation to reduce dimensions with PCA on data matrix X:
  T = X * W_R,  where columns of W_R are first n eigenvectors of X^T X 
* Do a PCA in python:
  pca = sklearn.decomposition.PCA(n_components=n)(training_matrix)
* Once you have a PCA instance in python, transform data:
  pca_ed = pca.transform(new_data)
* SVD:
  Singular value decomposition, analogous to eigendecomposition for rectangular matrices
* In SVD, (m, n) matrix M is factored:
  M = U S V^*, S is (m, n) diagonal, U (m, m) unitary, V (n, n) unitary
* Unitary matrices in SVD of matrix M:
  Consist of eigenvectors of M^* M and M M^* (note that they are Hermitian)
* Singular values of matrix M:
  Non-negative square roots of eigenvalues of either M^* M or M M^*
* Feature hashing:
  Hashing terms instead of creating an explicit vocabulary, creates indices
* Get dataset from a TFRecord file:
  ds = tf.data.TFRecordDataset(path)
* Parsing function for single TFRecordDataset record:
  Create dictionary {feature/label: tf.FixedLenFeature/VarLenFeature}, call tf.parse_single_example(record, dictionary), return ({features: values}, label) using returned dictionary
* Specify a fixed length feature type in tensorflow:
  tf.FixedLenFeature(shape=[...], dtype, default)
* tf.parse_single_examples(record, dictionary) returns:
  A dictionary with keys from dictionary as keys and tensors of the appropriate type as values
* Specify a variable length feature type in tensorflow:
  tf.VarLenFeature(dtype)
* Parse TFRecordDataset ds:
  Apply parsing function with ds.map(parse_fn)
* Make a categorical feature column with a vocabulary list:
  tf.feature_column.categorical_column_with_vocabulary_list(key, vocabulary_list)
* Make sparse feature column work for input in DNNClassifier:
  Wrap in either indicator_column or embedding_column
* Add embedding layer in tensorflow:
  Create embedding column, tf.feature_column.embedding_column(categorical, n_dims)
* Save dataset from web site using keras:
  tf.keras.utils.get_file(file, url)
* Default location of saved dataset from keras:
  ~/.keras/
* Create categorical column from a vocabulary file:
  tf.feature_column.categorical_column_with_vocabulary_file(key, file)
* Two types of training of production ML system:
  Static model (trained offline) and dynamic model (trained online)
* Positives of static model:
  Easy to build, test, iterate
* Drawbacks of static model:
  Can become stale if situation changes
* Drawbacks of dynamic model:
  Much more monitoring required, model rollback, data quarantine
* Positives of dynamic model:
  Won't become stale quickly
* When to use static vs dynamic model:
  Use static when the data won't change over time
* Two types of inference:
  Online and offline
* Strengths of online inference:
  Good for long-tail, since predicts any new item
* Weakness of online inference:
  Usually need low latency (keep it simple), need to monitor more
* Strengths of offline inference:
  Not as computationally limited, can do sanity checks, quick lookup
* Weaknesses of offline inference:
  Need examples already (not good for long-tail stuff), longer to update
* Questions to ask about input data:
  Correlated, reliable version necessitate feedback: correlation, reliability, versioning, necessity, feedback
* Reason to think about correlation vs causation in data:
  Correlation (as opposed to causation) is more subject to change over time
* Data versioning:
  Format, method of producing data may change discontinuously
* Feedback in data:
  Happens if your model actually influences the data
* Examples of human biases in data:
  Overgeneralized reporting selects out-group confirmed automatons: overgeneralization, reporting bias, selection bias, out-group homogeneity bias, confirmation bias, automation bias
* Create categorical feature with a hash bucket:
  tf.feature_column.categorical_column_with_hash_bucket(feature, hash_bucket_size)
* Reporting bias:
  Reported examples don't match real-world frequencies (interesting examples overrepresented)
* Selection bias:
  Chosen examples don't match real-world frequencies
* Subtypes of selection bias:
  Coverage (initial selection), non-response (only some people respond), and sampling (incorrect randomization) bias
* Overgeneralization:
  Generalizing too much from limited data
* Group attribution bias:
  Incorrectly generalizing properties of individuals to groups
* Two examples of group attribution bias:
  In-group bias (preferring members in your group), out-group homogeneity bias
* Out-group homogeneity bias:
  Thinking that people in other groups are more similar
* Confirmation bias:
  Example of implicit bias, paying attention to information that confirms previous opinions
* Implicit bias:
  Wrongly generalizing personal experience
* Experimenter's bias:
  Training model until it conforms with hypothesis, an implicit bias
* Automation bias:
  Assuming automated systems are more trustworthy
* Ways to mitigate bias:
  Missing experts distribute implicit context outcomes to subgroups: Missing data, experts, distribution with outliers, implicit assumptions, context when publishing, interpret Outcomes (how would human treat task, social cues), subgroups
* Red flags for bias:
  Missing data, unexpected feature values, skewed data
* Drop rows with any missing data in pandas:
  df.dropna(how="any", axis=0)
* Find correlation matrix in pandas:
  df.corr()
* Plot categorical histogram in python:
  Use seaborn, seaborn.countplot(pd.Series)
* Label leakage:
  Label information "leaks" into features in ways that can't be used for prediction on new features (e.g. cancer hospitals)
* Guidelines for first steps in an ML system:
  Simple pipes make metric input configurations fail: keep first model simple, check data pipeline, use simple evaluation metric, own and monitor input features, treat configuration as code (check in/review), keep track of experiments (especially failures)
* 

#  LocalWords:  tf ds fn xs ys iid std argmax dataframe exp ROC AUC
#  LocalWords:  AUROC TP FP BINBO logit logits ids int ReLU fcs ReLUs
#  LocalWords:  DDNRegressor memoizing Adagrad Softmax len NxM PCA xy
#  LocalWords:  orthogonalizable QM MSE SGD cols col lr sess overfit
#  LocalWords:  bucketized fpr tpr NP wrt pca SVD TFRecordDataset df
#  LocalWords:  TFRecord dict DNNClassifier keras crvnf versioning pd
#  LocalWords:  overgeneralization MUCBOMS FixedLenFeature dtype corr
#  LocalWords:  VarLenFeature ROSOCA dropna seaborn countplot ORSOCA
